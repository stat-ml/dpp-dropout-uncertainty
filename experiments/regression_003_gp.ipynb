{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from os import path\n",
    "from argparse import ArgumentParser\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import gpytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from alpaca.utils.ue_metrics import uq_ll\n",
    "from alpaca.ue.masks import BasicBernoulliMask, DPPMask\n",
    "import alpaca.nn as ann\n",
    "from alpaca.utils.model_builder import uncertainty_mode, inference_mode\n",
    "from alpaca.utils.datasets.builder import build_dataset\n",
    "\n",
    "\n",
    "def manual_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "save_dir = Path('data/regression_4')\n",
    "\n",
    "### params\n",
    "lengthscale = 1e-2\n",
    "T = 10000\n",
    "\n",
    "\n",
    "def main(name, repeats, batch_size, sampler):\n",
    "    manual_seed(42)\n",
    "    dataset = build_dataset(name, val_split=0)\n",
    "    x, y = dataset.dataset('train')\n",
    "    N = x.shape[0] * 0.9  # train size\n",
    "\n",
    "    #%%\n",
    "    # best_tau, best_dropout = 0.01, 0.05\n",
    "    best_tau, best_dropout = select_params(x, y, N, batch_size, name, sampler)\n",
    "\n",
    "    #%%\n",
    "    vanilla_rmse = []\n",
    "    vanilla_ll = []\n",
    "    mc_rmse = []\n",
    "    mc_ll = []\n",
    "\n",
    "    for i in range(repeats):\n",
    "        manual_seed(42 + i)\n",
    "        print(i)\n",
    "        x_train, y_train, x_test, y_test, y_scaler = split_and_scale(x, y)\n",
    "\n",
    "        model = build_and_train(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            500,\n",
    "            best_tau,\n",
    "            best_dropout,\n",
    "            N,\n",
    "            batch_size,\n",
    "            split_num=i,\n",
    "            name=name,\n",
    "            sampler=sampler\n",
    "        )\n",
    "\n",
    "        error, ll = rmse_nll(model, 1, x_test, y_test, y_scaler, tau=best_tau, dropout=False)\n",
    "        vanilla_rmse.append(error)\n",
    "        vanilla_ll.append(ll)\n",
    "\n",
    "        error, ll = rmse_nll(model, T, x_test, y_test, y_scaler, tau=best_tau, dropout=True)\n",
    "        mc_rmse.append(error)\n",
    "        mc_ll.append(ll)\n",
    "\n",
    "    print('vanilla')\n",
    "    print(np.mean(vanilla_rmse), np.std(vanilla_rmse))\n",
    "    print(np.mean(vanilla_ll), np.std(vanilla_ll))\n",
    "    print('mc')\n",
    "    print(np.mean(mc_rmse), np.std(mc_rmse))\n",
    "    print(np.mean(mc_ll), np.std(mc_ll))\n",
    "\n",
    "    # plt.boxplot(vanilla_rmse)\n",
    "    # plt.boxplot(mc_rmse)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, dropout_value, sampler):\n",
    "        super().__init__()\n",
    "        if sampler == 'mc':\n",
    "            mask_class = DPPMask\n",
    "        else:\n",
    "            mask_class = BasicBernoulliMask\n",
    "\n",
    "        base_size = 64\n",
    "\n",
    "        self.dropout_1 = ann.Dropout(dropout_rate=dropout_value, dropout_mask=mask_class())\n",
    "        self.fc1 = nn.Linear(input_size, base_size)\n",
    "\n",
    "        self.dropout_2 = ann.Dropout(dropout_rate=dropout_value, dropout_mask=mask_class())\n",
    "        self.fc2 = nn.Linear(base_size, 2*base_size)\n",
    "\n",
    "        self.dropout_3 = ann.Dropout(dropout_rate=dropout_value, dropout_mask=mask_class())\n",
    "        self.fc3 = nn.Linear(2*base_size, 2*base_size)\n",
    "\n",
    "        self.dropout_4 = ann.Dropout(dropout_rate=dropout_value, dropout_mask=mask_class())\n",
    "        self.fc4 = nn.Linear(2*base_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        x = self.dropout_3(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        x = self.dropout_4(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def loader(x_array, y_array, batch_size):\n",
    "    dataset = TensorDataset(torch.Tensor(x_array), torch.Tensor(y_array))\n",
    "    return DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def rmse(values, predictions):\n",
    "    return np.sqrt(np.mean(np.square(values - predictions)))\n",
    "\n",
    "\n",
    "def rmse_nll(model, T, x_test, y_test, y_scaler, tau, dropout=True):\n",
    "    y_test_unscaled = y_scaler.inverse_transform(y_test)\n",
    "    model.eval()\n",
    "    if dropout:\n",
    "        uncertainty_mode(model)\n",
    "    else:\n",
    "        inference_mode(model)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = np.array([\n",
    "            y_scaler.inverse_transform(\n",
    "                model(torch.Tensor(x_test).to(device).double()).cpu().numpy()\n",
    "            )\n",
    "            for _ in range(T)\n",
    "        ])\n",
    "\n",
    "    y_pred = np.mean(y_hat, axis=0)\n",
    "    errors = np.abs(y_pred - y_test_unscaled)\n",
    "    ue = np.std(y_hat, axis=0) + 1/tau\n",
    "    ll = uq_ll(errors, ue)\n",
    "    # ll = np.mean((logsumexp(-0.5 * tau * (y_test_unscaled[None] - y_hat)**2., 0) - np.log(T)\n",
    "    #         - 0.5*np.log(2*np.pi) + 0.5*np.log(tau)))\n",
    "    return rmse(y_test_unscaled, y_pred), ll\n",
    "\n",
    "\n",
    "def split_and_scale(x, y):\n",
    "    # Load dat\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
    "\n",
    "    # Scaler\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train = y_scaler.fit_transform(y_train)\n",
    "    y_test = y_scaler.transform(y_test)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_scaler\n",
    "\n",
    "\n",
    "def build_and_train(x_train, y_train, epochs, tau, dropout_value, N, batch_size, split_num, name, sampler):\n",
    "    if split_num is None:\n",
    "        file_name = None\n",
    "    else:\n",
    "        file_name = save_dir / f\"{name}_{sampler}_{split_num}.pt\"\n",
    "\n",
    "    reg = lengthscale**2 * (1 - dropout_value) / (2. * N * tau)\n",
    "    train_loader = loader(x_train, y_train, batch_size)\n",
    "\n",
    "    model = Network(x_train.shape[1], dropout_value, sampler).to(device).double()\n",
    "    if file_name and path.exists(file_name):\n",
    "        model.load_state_dict(torch.load(file_name))\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=reg)\n",
    "        criterion = nn.MSELoss()\n",
    "        train_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                preds = model(x_batch.to(device).double())\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(y_batch.to(device).double(), preds)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "            if epoch % 10 == 0:\n",
    "                train_losses.append(np.mean(losses))\n",
    "        if file_name:\n",
    "            pass\n",
    "            # torch.save(model.state_dict(), file_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def select_params(x, y, N, batch_size, name, sampler):\n",
    "    file_name = save_dir/f\"{name}_params_{sampler}.pickle\"\n",
    "\n",
    "    if path.exists(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        best_tau = params['tau']\n",
    "        best_dropout = params['dropout']\n",
    "    else:\n",
    "        x_train, y_train, x_test, y_test, y_scaler = split_and_scale(x, y)\n",
    "        results = []\n",
    "        for local_tau in np.logspace(-4, 2, 14):\n",
    "            for local_dropout in [0.05, 0.2, 0.5]:\n",
    "                model = build_and_train(\n",
    "                    x_train, y_train, 50, local_tau, local_dropout, N, batch_size, None, name, sampler\n",
    "                )\n",
    "                error, ll = rmse_nll(model, 1, x_test, y_test, y_scaler, dropout=False, tau=local_tau)\n",
    "                results.append((ll, error, (local_tau, local_dropout)))\n",
    "                print(results[-1])\n",
    "\n",
    "        best_tau, best_dropout = sorted(results, key=lambda p: p[0])[-1][-1]\n",
    "        print(best_tau, best_dropout)\n",
    "        # with open(file_name, 'wb') as f:\n",
    "        #     params = {'tau': best_tau, 'dropout': best_dropout}\n",
    "            # pickle.dump(params, f)\n",
    "    return best_tau, best_dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# repeats = 1\n",
    "name = 'boston_housing'\n",
    "\n",
    "manual_seed(42)\n",
    "dataset = build_dataset(name, val_split=0)\n",
    "x, y = dataset.dataset('train')\n",
    "N = x.shape[0] * 0.9  # train size\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best_tau, best_dropout = 0.01, 0.05\n",
    "# best_tau, best_dropout = select_params(x, y, N, batch_size, name, sampler)\n",
    "\n",
    "# for i in range(repeats):\n",
    "manual_seed(42)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error as mse\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "def rmse_ll(true_y, prediction, y_scaler):\n",
    "    return np.square(\n",
    "        mse(\n",
    "            y_scaler.inverse_transform(true_y),\n",
    "            y_scaler.inverse_transform(prediction)\n",
    "        )\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, y_scaler = split_and_scale(x, y)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# for length_scale in np.logspace(-3, 1, 24):\n",
    "# for restarts in [0, 3, 5, 10, 20, 50]:\n",
    "restarts = 500\n",
    "length_scale = 1e-2\n",
    "kernel = C(1.0) * RBF(13*[length_scale])\n",
    "model = GaussianProcessRegressor(alpha=1e-3, kernel=kernel, n_restarts_optimizer=restarts)\n",
    "model.fit(x_train, y_train)\n",
    "error = rmse_ll(y_test, model.predict(x_test), y_scaler)\n",
    "print(error, restarts)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds, uq = model.predict(x_test, return_std=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred = np.mean(y_train) * np.ones(y_test.shape)\n",
    "rmse_ll(y_test, pred, y_scaler)\n",
    "\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 50\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_x = torch.linspace(0, 1, 51).cuda()\n",
    "model.eval().cuda()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}