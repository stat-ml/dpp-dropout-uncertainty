{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ood.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "f7Cmy4ZoVxHl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# !python -m pip install git+https://github.com/stat-ml/alpaca"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oowlIzsAgWm1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3b_TKwi_gjzF",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from alpaca.model.ensemble import MLPEnsemble\n",
    "from alpaca.model.mlp import MLP\n",
    "from alpaca.dataloader.builder import build_dataset\n",
    "from alpaca.uncertainty_estimator import build_estimator\n",
    "from alpaca.analysis.metrics import get_uq_metrics"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lLC9TQA1gltX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# scaling back\n",
    "def scale(train, val):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    val = scaler.transform(val)\n",
    "    return train, val, scaler\n",
    "\n",
    "\n",
    "def split_ood(x_all, y_all, percentile=10):\n",
    "    threshold = np.percentile(y_all, percentile)\n",
    "    ood_idx = np.argwhere(y_all > threshold)[:, 0]\n",
    "    x_ood, y_ood = x_all[ood_idx], y_all[ood_idx]\n",
    "    train_idx = np.argwhere(y_all <= threshold)[:, 0]\n",
    "    x_train, y_train = x_all[train_idx], y_all[train_idx]\n",
    "\n",
    "    return x_train, y_train, x_ood, y_ood\n",
    "\n",
    "\n",
    "def multiple_kfold(k, data_size, max_iterations):\n",
    "    kfold = KFold(k)\n",
    "    for i in range(max_iterations):\n",
    "        if i % k == 0:\n",
    "            data_idx = np.random.permutation(data_size)\n",
    "            idx_generator = kfold.split(data_idx)\n",
    "        train_idx, val_idx = next(idx_generator)\n",
    "        yield data_idx[train_idx], data_idx[val_idx]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bBqnhA9-gm20",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# reproducibility\n",
    "SEED = 10\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Itdkj_-DhtTn",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from alpaca.uncertainty_estimator.masks import build_masks, DEFAULT_MASKS\n",
    "from alpaca.model.ensemble import MLPEnsemble\n",
    "from alpaca.uncertainty_estimator import build_estimator\n",
    "from alpaca.analysis.metrics import get_uq_metrics"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aqbXuV_PkU_n",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# estimator that invokes low-level functions\n",
    "def estimate(name, x_val, y_val, model):\n",
    "  results = []\n",
    "  x_val_tensor = torch.DoubleTensor(x_val).cuda()\n",
    "  predictions = model(x_val_tensor).cpu().detach().numpy()\n",
    "\n",
    "  errors = np.abs(predictions - y_val)\n",
    "\n",
    "  unscale = lambda y : y_scaler.inverse_transform(y)\n",
    "  scaled_errors = unscale(predictions) - unscale(y_val)\n",
    "  rmse = np.sqrt(np.mean(np.square(scaled_errors)))\n",
    "\n",
    "  estimator = build_estimator(\n",
    "      'mcdue_masked', model, nn_runs=100, \n",
    "      dropout_mask=name, dropout_rate=config['dropout_uq'])\n",
    "\n",
    "  estimations = estimator.estimate(x_val_tensor)\n",
    "  acc, ndcg, ll = get_uq_metrics(estimations, errors,\n",
    "                                config['acc_percentile'],\n",
    "                                bins = [80, 95, 99]\n",
    "                                )\n",
    "  return [acc, ndcg, ll, rmse, name, str(estimations)]"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zPG7WycNmMV1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import pandas as pd"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MXp6vnODvqlR",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# TRAINING"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DAaKxeldgqUX",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "res = []\n",
    "for dataset_name in ['boston_housing', 'concrete',\n",
    "                     'energy_efficiency', \n",
    "                     'kin8nm',\n",
    "                     'naval_propulsion', 'ccpp',\n",
    "                     'red_wine', 'yacht_hydrodynamics'\n",
    "                     ]:\n",
    "  print(f'=={dataset_name}==')\n",
    "  dataset = build_dataset(dataset_name, val_split=0.0)\n",
    "  x_set, y_set = dataset.dataset('train')\n",
    "  config['layers'][0] = x_set.shape[-1]\n",
    "\n",
    "  for dim in range(x_set.shape[-1]):\n",
    "    for run in range(5):\n",
    "      # splitting the data\n",
    "      med = np.median(x_set[:,dim])\n",
    "      x_train = x_set[x_set[:, dim] < med]\n",
    "      y_train = y_set[x_set[:, dim] < med]\n",
    "      x_val = x_set[x_set[:, dim] >= med]\n",
    "      y_val = y_set[x_set[:, dim] >= med]\n",
    "      # random picking on train\\test\n",
    "      if np.random.random() > .5:\n",
    "        x_train, x_val = x_val, x_train\n",
    "        y_train, y_val = y_val, y_train\n",
    "\n",
    "      x_train, x_val, x_scaler = scale(x_train, x_val)\n",
    "      y_train, y_val, y_scaler = scale(y_train, y_val)\n",
    "\n",
    "      model = MLP(config['layers'])\n",
    "      train_opts = {}\n",
    "      # fitting\n",
    "      model.fit((x_train, y_train),\n",
    "                (x_val, y_val),\n",
    "                **train_opts)\n",
    "      # UE\n",
    "      for name in DEFAULT_MASKS + ['cov_dpp', 'cov_k_dpp']:\n",
    "        tmp = estimate(name, x_train, y_train, model)\n",
    "        res.append(tmp + [dataset_name, dim, run, 'train'])\n",
    "        tmp = estimate(name, x_val, y_val, model)\n",
    "        res.append(tmp + [dataset_name, dim, run, 'val'])\n",
    "      dfq = pd.DataFrame(res,\n",
    "                  columns = ['acc', 'ndcg', 'll',\n",
    "                              'rmse', 'mask_name', 'ues',\n",
    "                              'dataset_name', 'dim', 'run', 'split'\n",
    "                              ])\n",
    "      dfq.to_csv('fname.csv', index = None)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hllce-r-tINE",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# MAKING TABLES"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vSEjL11ZvvoK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nd2fs3MfwBfI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data = []\n",
    "erc = 0\n",
    "# postprocessing UE data\n",
    "for q, dfx in dfq.groupby(['mask_name', \n",
    "                          'dataset_name', 'dim', 'run']):\n",
    "    vals_train = []\n",
    "    vals_val = []\n",
    "    try:\n",
    "        # get train percentile values\n",
    "        ue_str = dfx[dfx.split == 'train'].ues.values[0]\n",
    "        for x in ue_str[1:-1].split():\n",
    "            vals_train.append(float(x))\n",
    "        \n",
    "        ue_str = dfx[dfx.split == 'val'].ues.values[0]\n",
    "        for x in ue_str[1:-1].split():\n",
    "            vals_val.append(float(x))\n",
    "        vals_val = np.array(vals_val)\n",
    "        \n",
    "        for perc in [80, 90, 95]:\n",
    "            med = np.percentile(vals_train, perc)\n",
    "            data.append([\n",
    "                dfx.dataset_name.values[0],\n",
    "                dfx.dim.values[0],\n",
    "                dfx.run.values[0],\n",
    "                dfx.mask_name.values[0],\n",
    "                perc,\n",
    "                100*sum(vals_val > med)/len(vals_val) # scaling itself\n",
    "            ])\n",
    "    except Exception as e:\n",
    "        pass"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WhrZLc-UwExx",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "df_perc = pd.DataFrame(data,\n",
    "                  columns = ['dataset_name', 'dim', 'run',\n",
    "                            'mask_name', 'perc', 'ratio'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5tbDRE1Bw5PK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# reformatting for latex tables\n",
    "data2 = []\n",
    "for vals, df_temp in df_perc.groupby(['dataset_name',\n",
    "                                  'mask_name', 'perc'\n",
    "                                 ]):\n",
    "    dataset, mask, perc = vals\n",
    "    data2.append([dataset, mask, perc, f'{df_temp.ratio.mean():.1f}±{df_temp.ratio.std():.1f}'])\n",
    "dfres = pd.DataFrame(data2, \n",
    "                    columns = ['dataset', 'mask', 'percentile', 'ood_ratio'])"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "al_v3j4ixExI",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# output\n",
    "for dset in dfres.dataset.unique():\n",
    "    print(dset)\n",
    "    display(\n",
    "    dfres[(dfres.dataset == dset) & (dfres['mask'].isin(['mc_dropout',\n",
    "                                                      'ht_leverages',\n",
    "                                                      'ht_dpp', \n",
    "                                                      'cov_k_dpp'\n",
    "                                                     ]))]\\\n",
    "        .pivot(index='percentile', columns='mask', values='ood_ratio')\\\n",
    "        [['mc_dropout',\n",
    "                                                      'ht_leverages',\n",
    "                                                      'ht_dpp', \n",
    "                                                      'cov_k_dpp'\n",
    "                                                     ]]\n",
    "    )"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}